---
id: '013-nodejs-kafka-for-msa'
title: '[Node.js] Kafka로 만들어보는 MSA'
date: '2026-01-19'
description: 'MSA 전환을 위한 Kafka 기반 EDA 구축 및 Outbox 패턴 구현기'
---

# [Node.js] Kafka로 만들어보는 MSA

사이드 프로젝트를 진행하다보니, 차후 각 서비스가 필요할 때마다 독립적으로 가져다 쓸 수 있는 MSA에 대한 욕심이 생겼습니다.

이에 따라 분리된 서비스들을 유기적으로 잇기 위해 EDA(이벤트 기반 아키텍처)를 구상하게 되었고, 그 핵심 역할을 수행할 매개체로 Kafka를 선택했습니다.<br />
사실 Kafka는 현재 프로젝트 규모에는 과한 스펙이지만, 큰 규모의 회사들은 다 도입하는 기술인 만큼 엔지니어로서의 욕심을 가지고 진행하게 됐습니다.

이 글은 적용 과정에서 알게 된 점들을 공유하기 위한 포스팅입니다.

---

## Kafka란

Kafka는 LinkedIn에서 개발하고 Apache에서 관리하는 오픈소스 분산 이벤트 스트리밍 플랫폼입니다.

구조적으로 데이터를 생성하는 측(Producer)과 소비하는 측(Consumer) 사이의 의존성을 끊어내어, 시스템이 대용량 데이터를 실시간으로 막힘없이 소화할 수 있도록 합니다.

즉, Kafka는 MSA 환경에서 `A 서비스`가 생성한 이벤트가 `B 서비스`로 자연스럽게 흘러가도록 중계하는 역할을 수행합니다.

---

## Kafka 구조도 및 구성요소

우선 Kafka를 지탱하는 핵심 구성 요소들을 먼저 살펴보고, 이들이 어떻게 유기적으로 맞물려 돌아가는지 알아보겠습니다.
<br /><br />

#### Kafka 구조도

![013-nodejs-kafka-for-msa.png](/images/013-nodejs-kafka-for-msa.png)
*출처: [Velog 기술 블로그](https://velog.io/@jwpark06/Kafka-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EA%B5%AC%EC%A1%B0-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0)*

<br />

#### Kafka 구성요소

- **Producer**: 메시지를 생성하여 발송하는 주체로, 본 프로젝트의 `A 서비스`가 이에 해당합니다.
- **Zookeeper/KRaft**: 분산된 브로커들의 상태를 관리하고 리더 선출 등 클러스터 전체를 조율하는 관리자입니다.
- **Kafka Cluster**: 고가용성을 위해 여러 대의 브로커(서버)가 모여 하나의 시스템처럼 동작하는 서버 집합입니다.
- **Topic**: 메시지가 저장되는 논리적인 채널로, 파일 시스템의 폴더나 DB의 테이블과 같은 개념입니다.
- **Partition**: 병렬 처리를 통한 성능 향상을 위해 하나의 Topic을 물리적으로 나눈 데이터 분산 단위입니다.
- **Consumer Group**: 하나의 토픽 데이터를 안전하게 나누어 처리하기 위해 묶인 컨슈머들의 논리적 집합으로, 메시지를 구독하는 `B 서비스`를 의미합니다.
- **Consumer**: 파티션에 쌓인 메시지를 가져와 실제로 처리하는 주체이며, 스케일 아웃된 `B 서비스의 각 인스턴스`를 의미합니다.

---

## Kafka 구축

#### Kafka 아키텍처 및 인프라 설계

이론을 익혔으니, 이제 Kafka 클러스터를 로컬 개발 환경에 구축해보겠습니다.

과거에는 Kafka를 띄우려면 Zookeeper라는 관리자를 같이 띄워야 했지만, 최신 Kafka(3.3+)부터는 KRaft 모드가 상용화되어 Zookeeper 없이 Kafka 자체적으로 메타데이터 관리가 가능해졌습니다.
이에 따라 아키텍처의 간결함과 관리 편의성을 위해 Zookeeper를 배제하고, KRaft 모드로 3개의 브로커를 구성하여 아래와 같은 인프라 사양을 설계했습니다.

- **Broker**: 3 (ISR 및 Quorum 최소 조건 충족)
- **Broker Replication Factor**: 3 (브로커 수와 동일하게 설정하여 모든 브로커에 데이터를 복제하여 장애 대응)
- **Topic**: N (도메인별 유연한 토픽 생성)
- **Partition**: 3 (브로커 수의 N배수로 설정하여 데이터 균등 분산 및 처리 최적화)
- **Consumer Group**: 2 (실제 서비스 수 만큼 설정, Chat & Payment Service)
- **Consumer**: 6 (각 Consumer Group 당 파티션 수만큼 스케일 아웃)
<br />

또한, 분산 환경에서의 완벽한 데이터 정합성을 보장하기 위해 `Transactional Outbox Pattern`을 채택했으며, 이를 구현하기 위해 인프라에 `Kafka Connect(Debezium)`를 사용했습니다.

해당 아키텍처는 비즈니스 데이터와 이벤트를 하나의 트랜잭션으로 묶어 Outbox 테이블에 저장함으로써 원자성을 보장하고, 이후 Kafka Connect(Debezium)이 DB의 트랜잭션 로그(WAL)를 감지하여 비동기로 Kafka에 전송하는 방식입니다.

이를 통해 분산 시스템의 고질적인 난제인 이중 쓰기(Dual-Write) 문제를 다음과 같이 해결할 수 있습니다.

```javascript
// [Bad] 분산환경에서 가끔 볼 수 있는 이중 쓰기(Dual-Write) 
await db.transaction(async (conn) => {
    await kafkaProducer.emit('user.created', userDto); 

    await conn.save(User, userDto);
});
```

```javascript
// [Good] Transactional Outbox Pattern & Kafka Connect (Debezium)을 적용하여 원자성 보장
await db.transaction(async (conn) => {
    await conn.save(Outbox, {topic: 'user.created', payload: JSON.stringify(userDto)});

    await conn.save(User, userDto);
});
```
<br /><br />

#### IaC 기반 인프라 및 DB 구현

앞서 설계한 인프라 사양을 바탕으로, 실제 동작하는 환경을 Docker Compose로 구현해보겠습니다.<br />
구현 순서는 [Kafka Cluster 설정] -> [DB 및 Outbox 테이블 구성] -> [Debezium Connector 설정] -> [Kafka Connect 설정] 순으로 진행됩니다.<br />
(각 옵션 설명은 주석 참고)
<br />

**1. Kafka Cluster 설정**

KRaft 모드로 동작하는 3개의 브로커를 정의합니다.

```yaml
# ------------------------------------------------------------
# Kafka Cluster (KRaft)
# ------------------------------------------------------------
x-kafka-broker-environment: &kafka-broker-environment
  CLUSTER_ID: "8hfLpvRrIVKfjyDbhM6e4g" # Kafka Broker들을 하나의 Cluster ID 로 묶어줌 (22글자 제한)
  KAFKA_PROCESS_ROLES: "broker,controller" # Kafka Broker 투잡 처리

  # CONTROLLER:	관리자 통로 - 브로커끼리 리더를 뽑고 클러스터를 유지할 때 사용
  # PLAINTEXT: 일반 통로 - 브로커들끼리 데이터를 서로 복제할 때 사용
  # PLAINTEXT_HOST: 손님 통로 - 타 서비스가 데이터를 보내려고 찾아오는 통로 (외부 포트)
  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT" # Kafka가 알아들을 수 있는 4가지 프로토콜과 커스텀 네이밍을 매핑한다.
  KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
  KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"

  # 클러스터가 시작될 때, 이 명단을 보고 접속을 시도하여 Raft 합의 알고리즘(Quorum)을 돌려서 브로커 리더를 선출함 (과반수가 살아있어야 클러스터가 켜짐)
  # 형식은 다음과 같다. [KAFKA_CFG_NODE_ID]@[docker-compose 명령어 타겟][각 브로커의 CONTROLLER 포트]
  KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka-broker-1:9091,2@kafka-broker-2:9091,3@kafka-broker-3:9091"

  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3 # Kafka Offset을 몇개의 브로커에서 저장할지 지정 (안정성을 위해 브로커 개수만큼 설정)
  KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3 # Kafka의 트랜잭션 원리에서 Coordinator 장부(__transaction_state)를 몇 개의 브로커에 복제해서 저장할지 결정 (안정성을 위해 브로커 개수만큼 설정)

  KAFKA_DEFAULT_REPLICATION_FACTOR: 3 # Debezium이 토픽을 만들 때, 브로커 복제본 개수를 지정하지 않으면 적용하는 값

  KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2 # Kafka 트랜잭션 발송 시, Coordinator가 장부를 기록할 때, 최소 몇 대의 Broker가 '저장 완료'라고 응답해야 하는가를 정하는 기준값 (= Math.floor(브로커 개수/2) + 1)
  KAFKA_MIN_INSYNC_REPLICAS: 2 # Kafka 일반 발송 시, 데이터 유실을 방지하기 위해, 최소 몇 대의 Broker가 '저장 완료'라고 응답해야 하는가를 정하는 기준값 (= Math.floor(브로커 개수/2) + 1)

  KAFKA_NUM_PARTITIONS: 3 # 토픽 생성 시 파티션 개수를 명시하지 않았거나 자동 생성될 때 적용되는 파티션 개수의 기본값
  KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true" # 존재하지 않는 토픽에 메시지를 보냈을 때 브로커가 알아서 토픽을 만들지 결정하는 허용 스위치 (현재는 개발용이기에 true)
  KAFKA_LOG_DIRS: "/tmp/kafka-logs" # Kafka 브로커 별 로그 창고

  # 로컬을 위한 설정값
  KAFKA_HEAP_OPTS: "-Xms400M -Xmx400M" # Kafka 브로커 별 메모리 용량 (운영환경은 1G 이상 권장)
  KAFKA_REPLICA_LAG_TIME_MAX_MS: 60000 # ISR 목록에서 방출되기 전까지 봐주는 유예 시간
  KAFKA_CONTROLLER_QUORUM_REQUEST_TIMEOUT_MS: 60000 # 컨트롤러(관리자) 노드들끼리 투표나 데이터를 주고받을 때, 응답을 기다리는 최대 시간
  KAFKA_REQUEST_TIMEOUT_MS: 60000 # 브로커가 클라이언트나 다른 브로커에게 요청을 보냈을 때, 처리가 완료되기를 기다리는 시간

x-kafka-broker-base: &kafka-broker-base
  image: confluentinc/cp-kafka:7.6.0
  networks:
    - kafka-network
  restart: always
  healthcheck:
    test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list"]
    interval: 10s
    timeout: 5s
    retries: 5

services:
  kafka-broker-1:
    <<: *kafka-broker-base
    hostname: kafka-broker-1
    container_name: kafka-broker-1
    ports:
      - "9092:9092"
    environment:
      <<: *kafka-broker-environment
      KAFKA_NODE_ID: 1
      KAFKA_LISTENERS: "CONTROLLER://kafka-broker-1:9091,PLAINTEXT://kafka-broker-1:29092,PLAINTEXT_HOST://0.0.0.0:9092" # "Broker가 통신을 받기 위해 특정 포트들을 실제로 열고 대기하는 '수신 창구'
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka-broker-1:29092,PLAINTEXT_HOST://localhost:9092" # "클라이언트가 나(Broker)를 찾아올 수 있도록, Broker 가 메타데이터에 담아 건네주는 '논리적 공식 명함'
      # TODO: 0.0.0.0, localhost 는 개발용
    volumes:
      - ./tmp/kafka-logs/kafka-broker-1:/tmp/kafka-logs
    healthcheck:
      test:
        ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka-broker-2:
    <<: *kafka-broker-base
    hostname: kafka-broker-2
    container_name: kafka-broker-2
    ports:
      - "9093:9093"
    environment:
      <<: *kafka-broker-environment
      KAFKA_NODE_ID: 2
      KAFKA_LISTENERS: "CONTROLLER://kafka-broker-2:9091,PLAINTEXT://kafka-broker-2:29092,PLAINTEXT_HOST://0.0.0.0:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka-broker-2:29092,PLAINTEXT_HOST://localhost:9093"
    volumes:
      - ./tmp/kafka-logs/kafka-broker-2:/tmp/kafka-logs
    healthcheck:
      test:
        ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9093 --list"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka-broker-3:
    <<: *kafka-broker-base
    hostname: kafka-broker-3
    container_name: kafka-broker-3
    ports:
      - "9094:9094"
    environment:
      <<: *kafka-broker-environment
      KAFKA_NODE_ID: 3
      KAFKA_LISTENERS: "CONTROLLER://kafka-broker-3:9091,PLAINTEXT://kafka-broker-3:29092,PLAINTEXT_HOST://0.0.0.0:9094"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka-broker-3:29092,PLAINTEXT_HOST://localhost:9094"
    volumes:
      - ./tmp/kafka-logs/kafka-broker-3:/tmp/kafka-logs
    healthcheck:
      test:
        ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9094 --list"]
      interval: 10s
      timeout: 5s
      retries: 5
```
<br />

**2. DB 및 Outbox Table 구성**

Transactional Outbox Pattern의 핵심인 `kafka_outbox` 테이블을 생성합니다.

```sql
ALTER SYSTEM SET wal_level = logical;       -- Debezium이 DB의 로그를 읽을 수 있도록 wal_level을 logical로 변경 (설정 후 DB 재시작 필수)
ALTER ROLE user_you_want WITH REPLICATION;  -- 권한 설정
ALTER ROLE user_you_want WITH LOGIN;

CREATE TABLE public.kafka_outbox (
    id UUID PRIMARY KEY,
    topic TEXT NOT NULL,                    -- 토픽 이름 결정
    aggregate_id TEXT NOT NULL,             -- 해당 데이터를 어느 파티션에 넣을지 결정하는 기준값이자, 동일한 ID를 가진 놈들을 한 줄로 세워 순서를 보장하는 파티션 키
    event_type TEXT NOT NULL,               -- 직접 넣는 이벤트 타입
    event_payload JSONB,                    -- 직접 넣는 이벤트 페이로드
    created_at TIMESTAMP DEFAULT NOW()
);

GRANT ALL PRIVILEGES ON TABLE public.kafka_outbox TO user_you_want;
```
<br />

**3. Debezium Connector 설정**

Debezium이 `kafka_outbox` 테이블을 감지하여 Kafka로 전송하는 규칙을 직접 설정합니다.<br />
이 설정을 통해 단순한 DB 행의 변경 사항이 우리가 원하는 포맷의 Kafka 메시지로 변환되어 정확한 토픽과 파티션으로 라우팅 됩니다.

```json
{
  "name": "outbox-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "tasks.max": "1",
    "database.hostname": "host.docker.internal",
    "database.port": "5432",
    "database.user": "user_you_want",
    "database.dbname": "postgres",
    "plugin.name": "pgoutput",
    
    "table.include.list": "public.kafka_outbox",
    "topic.prefix": "emotiontree",
    
    "transforms": "outbox",
    "transforms.outbox.type": "io.debezium.transforms.outbox.EventRouter",

    // 토픽 라우팅: 'topic' 컬럼의 값을 실제 Kafka 토픽 이름으로 사용
    "transforms.outbox.route.by.field": "topic",
    "transforms.outbox.route.topic.replacement": "${routedByValue}",

    // 메시지 본문: 'event_payload' 컬럼의 값을 Kafka 메시지 Body(Value)로 사용
    "transforms.outbox.table.field.event.payload": "event_payload",

    // 파티셔닝 키: 'aggregate_id' 컬럼을 Kafka 메시지 Key로 사용하여 순서 보장
    "transforms.outbox.table.field.event.key": "aggregate_id",
    "transforms.outbox.route.topic.partition.by": "aggregate_id",

    // 헤더 설정: 'event_type' 컬럼은 메시지 본문이 아닌 Kafka Header('eventType')에 포함
    "transforms.outbox.table.fields.additional.placement": "event_type:header:eventType",

    "tombstones.on.delete": "false"
  }
}
```
<br />

**4. Kafka Connect 설정**

마지막으로, 위에서 작성한 Connector 설정을 실행할 Kafka Connect 컨테이너입니다.

```yaml
  # ------------------------------------------------------------
  # Kafka Connect (Debezium)
  # ------------------------------------------------------------
  kafka-connect-1:
    image: debezium/connect:2.5
    hostname: kafka-connect-1
    container_name: kafka-connect-1
    depends_on:
      kafka-broker-1:
        condition: service_healthy
      kafka-broker-2:
        condition: service_healthy
      kafka-broker-3:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      GROUP_ID: kafka-connect-group-1
      BOOTSTRAP_SERVERS: kafka-broker-1:29092,kafka-broker-2:29092,kafka-broker-3:29092 # Kafka Cluster 값 참고

      # 동일 CONNECT_GROUP_ID 내의 다른 워커들이 찾아올 수 있는 현재 Kafka Connect 워커의 주소
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect-1
      CONNECT_REST_PORT: 8083

      # Kafka Connect 운영을 위한 시스템 토픽 설정
      CONFIG_STORAGE_TOPIC: kafka-connect-configs # Kafka Connect의 모든 설정 정보를 저장하는 중앙 저장소
      OFFSET_STORAGE_TOPIC: kafka-connect-offsets # Kafka Connect가 읽고 있는 데이터의 위치 정보를 저장
      STATUS_STORAGE_TOPIC: kafka-connect-status # Kafka Connect의 현재 생사 확인 및 상태를 공유
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3 # 데이터 복제를 위한 브로커 복제 수 (= 브로커 수)
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3 # 데이터 복제를 위한 브로커 복제 수 (= 브로커 수)
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3 # 데이터 복제를 위한 브로커 복제 수 (= 브로커 수)
      CONNECT_CONFIG_STORAGE_PARTITIONS: 1 # 커넥터의 설정 정보를 저장하는 토픽의 파티션 개수 (설정 변경의 순서 보장을 위해 1로 지정 필수)
      CONNECT_OFFSET_STORAGE_PARTITIONS: 25 # Source Connector가 데이터를 어디까지 읽었는지 Offset을 기록하는 토픽의 파티션 개수 (시스템 토픽 설정을 위해 파티션 수를 별도로 지정)
      CONNECT_STATUS_STORAGE_PARTITIONS: 5 # Connector와 Task들의 상태를 저장하는 토픽의 파티션 개수 (시스템 토픽 설정을 위해 파티션 수를 별도로 지정)

      # Kafka Connect를 통하는 데이터(ConnectRecord) 컨버터 정의
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter # ConnectRecord의 Key(식별자)를 어떤 형태로 직렬화해서 토픽에 저장할지 결정
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter # ConnectRecord의 Value(데이터 본문)를 어떤 형태로 직렬화하여 토픽에 저장할지 결정
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false" # 직렬화된 Key에 스키마 구조 정보(Metadata)를 포함할지 여부
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false" # 직렬화된 Value에 스키마 구조 정보(Metadata)를 포함할지 여부
    # Kafka Connect outbox 패턴을 위한 토픽 정의
    volumes:
      - ./connectors:/tmp/connectors
    # 컨테이너 실행 후 자동으로 위에서 정의한 Debezium Connector 설정을 로드하여 커넥터를 등록하도록 구현
    command: >
      /bin/bash -c "
        # 1. 기존 Debezium 실행 스크립트를 백그라운드(&)로 실행
        sleep 10;
        /docker-entrypoint.sh start &

        # 2. 서버가 뜰 때까지 내부에서 대기
        echo 'Kafka Connect wait booting...';
        while [ $$(curl -s -o /dev/null -w %{http_code} http://localhost:8083/) -ne 200 ]; do 
          sleep 5;
        done;

        # 3. Kafka Connect 등록 (outbox-connector.json 사용)
        echo 'Kafka Connector registering...';
        curl -v -X POST -H 'Content-Type: application/json' --data @/tmp/connectors/outbox-connector.json http://localhost:8083/connectors;

        # 4. 백그라운드에서 실행 중인 Connect 서버가 꺼지지 않도록 대기
        echo 'Kafka Connect started!';

        wait
      "
    networks:
      - kafka-network
```
<br />

**5. 컨테이너 실행 및 상태 확인**

```shell
❯ docker-compose up -d
❯ docker ps
CONTAINER ID   IMAGE                         COMMAND                   CREATED       STATUS                 PORTS                                         NAMES
60595ab477ff   debezium/connect:2.5          "/docker-entrypoint.…"   2 weeks ago   Up 2 weeks             0.0.0.0:8083->8083/tcp, [::]:8083->8083/tcp   kafka-connect-1
29f3e2d2adfd   confluentinc/cp-kafka:7.6.0   "/etc/confluent/dock…"   2 weeks ago   Up 2 weeks (healthy)   0.0.0.0:9094->9094/tcp, [::]:9094->9094/tcp   kafka-broker-3
9aa514e8d5b2   confluentinc/cp-kafka:7.6.0   "/etc/confluent/dock…"   2 weeks ago   Up 2 weeks (healthy)   0.0.0.0:9092->9092/tcp, [::]:9092->9092/tcp   kafka-broker-1
180f894d981f   confluentinc/cp-kafka:7.6.0   "/etc/confluent/dock…"   2 weeks ago   Up 2 weeks (healthy)   0.0.0.0:9093->9093/tcp, [::]:9093->9093/tcp   kafka-broker-2
```
<br /><br />

#### 백엔드 구현 및 Outbox 패턴 검증

컨테이너까지 모두 준비되었으므로, 실제로 DB에 넣기만 해도 Kafka로 메시지가 날아가서 출력되는지 검증해 보겠습니다.
<br />

**1. Producer**

현재 구조의 핵심은 애플리케이션이 Kafka 프로듀서를 직접 호출하는 것이 아니라, DB 트랜잭션의 일부로 Outbox 테이블에 데이터를 넣는 것입니다.<br />
따라서 별도의 백엔드 코드 작성 없이, SQL 실행만으로 이벤트를 발행해 보겠습니다.

```sql
INSERT INTO public.kafka_outbox (
    id,
    topic,
    aggregate_id,
    event_type,
    event_payload
) VALUES (
    gen_random_uuid(),
    'emotiontree.user.created',
    'user_id_12345',
    'UserCreated',
    '{"id": "user_id_12345", "email": "koy@example.com", "name": "Kwon O-young"}'::jsonb
);
```
<br />

**2. Consumer 구현**

Kafka로 날아온 메시지를 수신할 컨슈머입니다.<br />
NestJS로 구현되었으며, Header(Event Type)와 Body(Payload)가 분리되어 잘 들어오는지 확인합니다.

```typescript
@Injectable()
@Controller()
export class UserCreatedConsumer {

  @EventPattern('emotiontree.user.created') // Debezium이 보낸 토픽 구독
  async handleUserCreated(@Payload() message: any, @Ctx() context: KafkaContext) {
    const { offset, partition } = context.getMessage();
    const headers = context.getMessage().headers;
    
    console.log(`\n============== [Consumer] Event Received ==============`);
    console.log(`🔹 Topic: ${context.getTopic()}`);
    console.log(`🔹 Partition: ${partition} / Offset: ${offset}`);
    console.log(`🔹 Event Type (Header): ${headers['eventType']}`);
    console.log(`🔹 Payload:`, message);
    console.log(`=======================================================\n`);
  }
}
```
```shell
# 출력 확인
============== [Consumer] Event Received ==============
🔹 Topic: emotiontree.user.created
🔹 Partition: 1 / Offset: 0
🔹 Event Type (Header): UserCreated
🔹 Payload: {
  id: "a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11",
  email: "koy@example.com",
  createdAt: "2026-01-19T15:00:00.000Z"
}
=======================================================
```

별도의 Producer 호출 없이 DB INSERT만으로 Kafka 메시지가 발행됨을 검증했습니다.<br />
이로써 비즈니스 로직과 이벤트 발행의 원자성을 보장하는 Transactional Outbox Pattern 환경 구축이 성공적으로 완료되었습니다.
<br /><br />

## 코드 참조

본 포스팅에서 다룬 Kafka 인프라 및 테스트 코드는 아래 링크에 공유해 두었습니다.<br />
(실제 서버 코드는 보안상의 이유로 Private으로 설정되었습니다.)

[코드 바로가기](https://github.com/koy6654/kafka-architecture)

---

## 그 외 Kafka 지식 정리

Kafka 환경 구축 과정에서 습득한 이론들을 메모장 형식으로 정리했습니다. 추후 아키텍처 설계나 트러블슈팅 시 참고할 수 있도록 기록합니다.
<br /><br />

#### Kafka 리더 종류

**브로커 리더 (Controller)**

- 대상: 브로커(Broker) 중 한 대
- 역할: 클러스터의 `관리자`입니다.
- 설명: 모든 브로커 중 단 한 대만 선정됩니다. 브로커가 새로 추가되거나 죽었을 때 이를 감지하고, 아래 설명할 '파티션 리더'를 누구로 할지 결정하여 모든 브로커에게 지시하는 역할을 합니다. (KRaft 모드에서는 이 투표 과정이 매우 중요합니다.)

**파티션 리더 (Partition Leader)**

- 대상: 특정 파티션의 복제본(Replica) 중 하나
- 역할: 실제 데이터의 `주인`입니다.
- 상세: 우리가 토픽에 메시지를 쓰고(Produce) 읽을(Consume) 때 직접 통신하는 대상입니다. 하나의 파티션이 여러 브로커에 복제되어 저장될 때, 그중 '리더'로 지정된 단 한 곳만이 쓰기/읽기 요청을 처리합니다. 나머지 복제본(Follower)은 리더의 데이터를 복사만 해둡니다.

**컨슈머 그룹 코디네이터 (Group Coordinator)**

- 대상: 브로커 중 한 대
- 역할: 컨슈머 그룹의 `반장`입니다.
- 상세: 컨슈머 그룹 내의 컨슈머들이 살아있는지 체크(Heartbeat)하고, 어떤 컨슈머가 어떤 파티션을 읽을지 할당(Rebalancing)해주는 브로커입니다. 각 컨슈머 그룹마다 담당 코디네이터 브로커가 지정됩니다.
<br /><br />

#### Kafka Offset

**정의**

파티션 내에 들어온 메시지에 부여되는 고유하고 순차적인 번호이자, 컨슈머가 어디까지 처리했는지 표시하는 책갈피입니다.
<br />

**처리 프로세스**

1. 새로운 데이터가 Broker에 들어옴
2. Broker가 Offset 100번 부여
3. Broker가 Log End Offset(LEO) 값을 101번으로 변경
4. 컨슈머가 Offset 100번을 Current Offset으로 부여하고 데이터를 가져옴
5. 컨슈머가 처리 완료 및 Commit 후 Committed Offset 101번 부여 (Committed Offset의 진짜 의미는 처리한 번호가 아니라, 여기서부터 다시 시작해라라는 시작점 좌표)
<br /><br />

#### Kafka 보안 프로토콜 종류

- PLAINTEXT: 암호화 없는 평문 (우리가 쓰는 것)
- SSL: 암호화 통신
- SASL_PLAINTEXT: 아이디/비번 인증 + 평문
- SASL_SSL: 아이디/비번 인증 + 암호화
<br /><br />

#### Kafka 트랜잭션 처리 5단계

**1. 트랜잭션 생성**
- Producer가 Broker로 트랜잭션 ID 전송
- Broker들은 Coordinator 선정 (Coordinator: 각 컨슈머 그룹의 브로커 리더)
- Coordinator는 해당 Producer에게 Producer ID, Epoch(세대 번호)를 발급

**2. 데이터 전송**

- Producer가 데이터를 보내기 시작하고, Coordinator는 자기 장부(\_\_transaction_state)에 현재 트랜잭션의 진행 중 상태, 토픽, 파티션 목록을 적어둠
- Producer는 토픽에 메시지를 작성, Broker 디스크에 저장됨

**3. 커밋 요청**

- Producer는 Coordinator에게 데이터 전송 완료 및 커밋 요청을 전달
- Coordinator는 자기 장부의 트랜잭션 상태에 PREPARE_COMMIT로 상태 값 변경 (이제 트랜잭션은 성공한 것으로 간주됨 - 브로커가 죽어도 재부팅하면 여기서부터 이어서 가능, 마치 PSQL WAL LOG와 같음)

**4. 커밋 마커 찍기**

- Coordinator가 적어둔 자기 장부에서 파티션 목록을 가져와 파티션 리더 Broker 에게 마커를 찍도록 요청
- 각 파티션 리더는 데이터 맨 끝에 Commit Marker라는 특수한 메시지를 추가

**5. 트랜잭션 종료 선언**

- Coordinator는 모든 파티션에 마커가 잘 박혔다는 보고를 받으면, 자기 장부에 최종적으로 COMPLETE_COMMIT을 기록
- 이제 해당 트랜잭션 ID에 대한 상태 정보를 메모리에서 지우고 트랜잭션 종료
<br /><br />

#### 실제 운영 환경 도입 시 필수 체크리스트

앞서 구현한 IaC는 로컬 개발용이기에, 실제 운영 환경에서는 고가용성과 안정성을 위해 아래 옵션들을 환경에 맞게 조정해야 합니다.
<br />

**안정성 및 복제 설정**
  - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR (오프셋 토픽 복제 수)
  - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR (트랜잭션 로그 복제 수)
  - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR (트랜잭션 최소 ISR)
  - KAFKA_MIN_INSYNC_REPLICAS (데이터 기록 최소 ISR)
  - KAFKA_NUM_PARTITIONS (기본 파티션 개수)

**토픽 관리 정책**
  - KAFKA_AUTO_CREATE_TOPICS_ENABLE (토픽 자동 생성 방지)

**브로커 리소스 설정**
  - KAFKA_HEAP_OPTS (JVM 힙 메모리 크기)
  - KAFKA_LOG_DIRS (데이터 저장 경로 - SSD 권장)

**호스트 및 도메인**
  - KAFKA_LISTENERS (내부 바인딩 주소)
  - KAFKA_ADVERTISED_LISTENERS (외부 접속 주소)

**데이터 보관 및 메시지 크기**
  - KAFKA_LOG_RETENTION_HOURS (데이터 보관 주기)
  - KAFKA_MESSAGE_MAX_BYTES (단일 메시지 최대 크기)
